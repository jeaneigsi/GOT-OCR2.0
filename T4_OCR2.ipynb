{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeaneigsi/GOT-OCR2.0/blob/main/T4_OCR2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy0fcqmnVVoW",
        "outputId": "b6a8dc7e-d618-4813-cf7a-5f8cba99976a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GOT-OCR2.0'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 163 (delta 60), reused 137 (delta 46), pack-reused 9 (from 1)\u001b[K\n",
            "Receiving objects: 100% (163/163), 8.46 MiB | 10.85 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ElvisClaros/GOT-OCR2.0.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/GOT-OCR2.0/GOT-OCR-2.0-master\n",
        "!pip install -e .\n",
        "!pip install ninja\n",
        "# !pip install flash-attn --no-build-isolation\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "OF-NOp-hVw0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "file_id = '1OQrXq_NB_QbJD9yPab6MSj0mcUD4DcrX'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output = 'GOT_weights.zip'\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "6XK-_brWcm5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/GOT_weights.zip -d /content"
      ],
      "metadata": {
        "id": "VBhBSeadYzBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0.py --model-name /content/GOT_weights/ --image-file /content/assets_aff2.png --type ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPgcIOO6apNJ",
        "outputId": "c4f0c7f2-5645-4c3f-898f-2ddbe901af5d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-15 20:43:04.334752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-15 20:43:04.354153: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-15 20:43:04.360007: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-15 20:43:05.598072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "<|im_start|>system\n",
            "You should follow the instructions carefully and explain your answers in detail.<|im_end|><|im_start|>user\n",
            "<img><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad></img>\n",
            "OCR: <|im_end|><|im_start|>assistant\n",
            "\n",
            "Proceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n",
            "positives (FP) via center points yet cannot solves the scenario\n",
            "that a center point of the third object lies exactly within the\n",
            "center of an FP. However, this scenario often occurs in some\n",
            "datasets, such as aircraft parked side by side in aerial images.\n",
            "CentripetalNet cannot address the object occlusion problem\n",
            "due to the center overlapping, e.g., pedestrian detection sce-\n",
            "nario. In summary, existing heuristic corner grouping meth-\n",
            "ods are dataset (COCO [Lin et al., 2014]) driven with poor\n",
            "generalization, but much less effort has been paid to solve\n",
            "it. To tackle this problem, we propose the Corner Afﬁnity,\n",
            "whose motivation is to cover all scenarios aforementioned\n",
            "and bring robust grouping capability for corner-guided detec-\n",
            "tor.\n",
            "Speciﬁcally, Corner Afﬁnity embeds three attributes for\n",
            "each target corner, i.e., locations, shapes, and semantic. Lo-\n",
            "cations and shapes are embedded in the structure afﬁnity (SA)\n",
            "that is a key part of the Corner Afﬁnity, in which we encode\n",
            "the shape (e.g., width and height) knowledge of each instance\n",
            "in its corner location via a strong supervision manner. SA\n",
            "is a ﬁnal model that is a ﬁnal model that is a ﬁnal model\n",
            "is not enough to accomplish grouping in some extreme sce-\n",
            "narios, such as two objects with similar shapes coincide. To\n",
            "this end, we devise the contexts afﬁnity (CA) part for Cor-\n",
            "ner Afﬁnity, in which we utilize the pull-push [Newell et\n",
            "al., 2017; Law and Deng, 2018] algorithm to mine high-level\n",
            "semantic similarity of the corresponding corner pairs to make\n",
            "coincident objects distinguishable. In the CA part, the afﬁnity\n",
            "of the corner pair is a ﬁnal model that is a ﬁnal model that is\n",
            "self-supervision manner with no need for a real ground-truth\n",
            "value. Besides, to make the two afﬁnities better interact, we\n",
            "devise the Corner Afﬁnity function, which runs as coupling\n",
            "CA and SA. In this way, SA and CA interplay so that even if\n",
            "the SA value of two corners belong to different instances is\n",
            "large, CA will decay it to make sure that the value of overall\n",
            "Corner Afﬁnity is not enough to be used.\n",
            "We select three datasets covering different detection sce-\n",
            "narios, i.e., COCO [Lin et al., 2014], Citypersons [Cords et\n",
            "al., 2016], and UCAS-AOD [Zhu et al., 2015], to verify the\n",
            "effectiveness of our design. Experimental results show that\n",
            "the proposed Corner Afﬁnity boosts AP of 5.8%, 35.8%, and\n",
            "17.2%, on the above three benchmarks upon CornerNaste-\n",
            "line, proving the solid effectiveness of our design. We hope\n",
            "the efﬁcient Corner Afﬁnity can attract more attention to pro-\n",
            "mote the development of bionic corner-guided detector.\n",
            "2\n",
            "Related Work\n",
            "2.1\n",
            "Center-guided Detector\n",
            "Center-guided detectors generally use potential center\n",
            "points/areas, acting as reference positives, to regress the ob-\n",
            "ject bounding box (e.g., height and width). Classical anchor-\n",
            "based detector belongs in this ﬁeld, which regards center as\n",
            "an attribute of the preset anchors. Faster R-CNN [Ren et al.,\n",
            "2015] popularizes the center-guided anchor mechanism in its\n",
            "Region Proposal Network (RPN). The aim of RPN is gener-\n",
            "ating a few of proposals from a set of candidate boxes that\n",
            "are encoded via their centers along with heights and widths,\n",
            "In addition, the design of the centered RPN makes the\n",
            "detector can be end-to-end trainable. Afterwards, the center-\n",
            "driven anchor boxes are widely used in RPN-based two-stage\n",
            "detectors. To further explore the efﬁciency of models, some\n",
            "center-guided one-stage detectors also appeared. They re-\n",
            "move the RPN and directly do regression and classiﬁcation\n",
            "at the centers of anchor boxes. For example, SSD [Liu et al.,\n",
            "2016] improves the performance via densely placing center\n",
            "anchors in multiple layers.\n",
            "Unlike the center-gpu and anchor-based detectors [Redmon\n",
            "and Farhadi, 2017; Liao et al., 2016; Lin et al., 2017b], FCOS\n",
            "[Tan et al., 2019] proposes a center-guided anchor-free man-\n",
            "agement strategy to generate the center areas of bounding boxes\n",
            "as positive samples and directly regresses four vectors (the\n",
            "distances from each pixel to the corresponding box’s bor-\n",
            "ders) without anchor guidance. Instead of four vectors, Rep-\n",
            "points [Yang et al., 2019] predicts a set of representative\n",
            "points in the center area (positive pixels).\n",
            "Despite the center-guided mechanism’s great success, it is\n",
            "actually difﬁcult to pinpoint the center of a box. This is be-\n",
            "cause a center requires to be determined via all four bound-\n",
            "aries of the object, needing four degrees of freedom.\n",
            "2.2\n",
            "Corner-guided Detector\n",
            "Corner-guided detectors usually predict corners via out-\n",
            "puting heatmaps, which gave same more bionic in the gen-\n",
            "eration of the object bounding box. Initially, when we want\n",
            "to obtain high-quality bounding boxes manually, each box is\n",
            "used to obtain the center to the bottom-right one and\n",
            "we rarely label the object box by a center point along with\n",
            "height and width. A corner plots used two boundaries (de-\n",
            "grees of freedom) to be determined.\n",
            "CornerNet [Law and Deng, 2018] detects objects by pre-\n",
            "dicting and grouping pairs of corner points. The grouping\n",
            "method in CornerNet is that if a top-left corner and a bottom-\n",
            "right corner belong to the same object, the distance between\n",
            "the center and the vectors will be small. To further optimize\n",
            "this grouping strategy, CenterNet [Duan et al., 2019] adds a\n",
            "prediction branch of corner points based on corners pairs esti-\n",
            "mation, making the center of the object from more complex match-\n",
            "ing. CentripetalNet [Dong et al., 2019] proposes a new cen-\n",
            "triptal grouping approach to matched corner points and\n",
            "achieves state-of-the-art performance.\n",
            "To sum up, corner grouping is meaningful yet challenge\n",
            "for this kind of detector. Our Corner Afﬁnity aims to produce\n",
            "more robust corner pairs to further advance the development\n",
            "of such human-like corner-guided detector.\n",
            "3\n",
            "Method\n",
            "As shown in Figure 2, the execution of Corner Afﬁnity re-\n",
            "quires an output of the corresponding corner afﬁnity map.\n",
            "Each (top-left or bottom-right) map is composed of three\n",
            "channels, i.e., two for encoding the structure afﬁnity (SA,\n",
            "blue arrow-lines) and one for the contexts afﬁnity (CA, green\n",
            "arrow-lines). The SA and CA interact via a devised function\n",
            "to be the overall Corner Afﬁnity. In the following subsections,\n",
            "we will detail the SA, CA, and Corner Afﬁnity function.\n",
            "3.1\n",
            "The Structure Afﬁnity\n",
            "The structure afﬁnity (SA) is a key part of Corner Afﬁnity,\n",
            "aiming to mine preliminary construction similarity of cor-\n",
            "1459\n",
            "\n"
          ]
        }
      ]
    }
  ]
}