{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeaneigsi/GOT-OCR2.0/blob/main/T4_OCR2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy0fcqmnVVoW",
        "outputId": "b6a8dc7e-d618-4813-cf7a-5f8cba99976a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GOT-OCR2.0'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 163 (delta 60), reused 137 (delta 46), pack-reused 9 (from 1)\u001b[K\n",
            "Receiving objects: 100% (163/163), 8.46 MiB | 10.85 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ElvisClaros/GOT-OCR2.0.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/GOT-OCR2.0/GOT-OCR-2.0-master\n",
        "!pip install -e .\n",
        "!pip install ninja\n",
        "# !pip install flash-attn --no-build-isolation\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "OF-NOp-hVw0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "file_id = '1OQrXq_NB_QbJD9yPab6MSj0mcUD4DcrX'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output = 'GOT_weights.zip'\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "6XK-_brWcm5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/GOT_weights.zip -d /content"
      ],
      "metadata": {
        "id": "VBhBSeadYzBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0.py --model-name /content/GOT_weights/ --image-file /content/assets_aff2.png --type format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPgcIOO6apNJ",
        "outputId": "d7e40db6-4608-44d4-c151-6c3a256a95de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-15 20:49:56.460746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-15 20:49:56.480966: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-15 20:49:56.486993: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-15 20:49:57.776016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "<|im_start|>system\n",
            "You should follow the instructions carefully and explain your answers in detail.<|im_end|><|im_start|>user\n",
            "<img><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad></img>\n",
            "OCR with format: <|im_end|><|im_start|>assistant\n",
            "\n",
            "positives (FP) via center points yet cannot solves the scenario that a center point of the third object lies exactly within the center of an FP. However, this scenario often occurs in some datasets, such as aircraft parked side by side in aerial images. CentripetalNet cannot address the object occlusion problem due to the center overlapping, e.g., pedestrian detection scenario. In summary, existing heuristic corner grouping methods or ads dataset (COCO [Lin et al., 2014]) driven with poor generalization, but much less effort has been paid to solve it. To tackle this problem, we propose the Corner Affinity, whose motivation is to cover all scenarios aforementioned and bring robust grouping capability for corner-guided detectors.\n",
            "Specifically, Corner Affinity embeds three attributes for each target corner, i.e., locations, shapes, and semantic. Loactions and shapes are embedded in the structure affinity (SA) that is a key part of the Corner Affinity, in which we encode the shape (e.g., width and height) knowledge of each instance in its corner location via a strong supervision manner. SA is a very simple way to learn the object to the object. We also then set only to accomplish grouping in some extreme scene ratios, such as two objects with similar shapes coincide. The density, and we deÔ¨Åve the contexts affinity (CA) part for Corner Affinity, in which we utilize the pull-push [Newell et al., 2017; Law and Deng, 2018] algorithm to mine high-level semantic similarity of the corresponding corner pairs to make coincident objects distinguishable. In the CA part, the different features of the object are the same as the object, which are self-supervision manner with none need for a real ground-truth value. Besides, to make the two affinities better interact, we devise the Corner Affinity function, which runs as coupling CA and SA. In this way, SA and CA interplay so that even if the SA value of two corners belong to different instances is large, CA will decay it to make sure that the value of overall CGD is not a real ground-truth value.\n",
            "We select three datasets covering different detection scenarios, also, COCO [Lin et al., 2014], Citypersons [Cords et al., 2016], and UCAS-AOD [Zhu et al., 2015], to verify the effectiveness of our design. Experimental results show that the proposed Corner Affinity boosts AP of \\(5.8 \\%, 35.8 \\%\\), and \\(17.2 \\%\\), on the above three benchmarks upon CornerNet baseline, proving the solid effectiveness of our design. We hope the efficient Corner Affinity can attract more attention to promote the development of bionic corner-guided detector.\n",
            "\\section*{2 Related Work}\n",
            "\\subsection*{2.1 Center-guided Detector}\n",
            "Center-guided detectors generally use potential center points/areas, acting as reference positives, to regress the object weight and box (e.g., height and width). Classical anchorbased detector belongs in this field, which regards center as an attribute of the preset anchors. Faster R-CNN [Ren et al., 2015] popularizes the center-guided anchor mechanism in its Region Proposal Network (RPN). The aim of RPN is generating a few of proposals from a set of candidate boxes that are encoded via their centers along with heights and widths. In addition, the design of the centered RPN makes the detector can be end-to-end trainable. Afterwards, the center- driven anchor boxes are widely used in RPN-based two-stage detectors. To further explore the efficiency of models, some center-guided one-stage detectors also appeared. They remove the RPN and directly do regression and classiÔ¨Åcation at the centers of anchor boxes. For example, SSD [Liu et al., 2016] improves the performance via densely placing center anchors in multiple layers.\n",
            "Unlike the center-gpu and anchor-based detectors [Redmon and Farhadi, 2017; Lui et al., 2016; Lin et al., 2017b], FCOS [Tian et al., 2019] proposes a center-guided anchor-free manner to improve pixels in center areas of bounding boxes as positive samples and directly regresses four vectors (the distances from each pixel to the corresponding box's borders) without anchor guidance. Instead of four vectors, Reppoints [Yang et al., 2019] predicts a set of representative points in the center area (positive pixels).\n",
            "Despite the center-guided mechanism's great success, it is actually difficult to pinpoint the center of a box. This is because a center requires to be determined via all four boundaries of the object, needing four degrees of freedom.\n",
            "\\subsection*{2.2 Corner-guided Detector}\n",
            "Corner-guided detectors usually predict corners via outputting heatmaps, which gave same more bionic in the generation of the object bounding box. Initially, when we want to obtain high-quality bounding boxes manually, each box is then connected to the bottom-right one and we rarely label the object box by a center point along with height and width. A corner-based two boundaries (decrees of freedom) to be determined.\n",
            "CornerNet [Law and Deng, 2018] detects objects by predicting and grouping pairs of corner points. The grouping method in CornerNet is that if a top-left corner and a bottomright corner belong to the same object, the distance between the corner points vectors will be small. To further optimize this grouping structure, CenterNet Duan et al., 2019] adds a prediction branch of corner points based on corners pairs estimation, making the correct image become triplets matching. CentripetalNet [Dong et al., 2019] proposes a new centerful grouping approach to matched areured corner points and achieves state-of-the-art performance.\n",
            "To sum up, corner grouping is meaningful yet challenge for this kind of detector. Our Corner Affinity aims to produce more robust corner pairs to further advance the development of such human-like corner-guided detector.\n",
            "\\section*{3 Method}\n",
            "As shown in Figure 2, the execution of Corner Affinity requires an output of the corresponding corner affinity maps. Each (top-left or bottom-right) map is composed of three channels, i.e., two for encoding the structure affinity (SA, blue arrow-lines) and one for the contexts affinity (CA, green arrows lines). The SA and CA interact via a devised function to be the overall Corner Affinity. In the following subsections, we will detail the SA, CA, and Corner Affinity function.\n",
            "\\subsection*{3.1 The Structure Affinity}\n",
            "The structure affinity (SA) is a key part of Corner Affinity, aiming to mine preliminary construction similarity of cor-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0.py --model-name /content/GOT_weights/ --image-file /content/assets_aff2.png --type ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKFVg1-oeH3O",
        "outputId": "f552cda8-6004-40a3-8baa-91d124bf11ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-15 20:56:08.688885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-15 20:56:08.720890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-15 20:56:08.730671: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-15 20:56:10.659839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "<|im_start|>system\n",
            "You should follow the instructions carefully and explain your answers in detail.<|im_end|><|im_start|>user\n",
            "<img><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad></img>\n",
            "OCR: <|im_end|><|im_start|>assistant\n",
            "\n",
            "Proceedings of the Thirty-First International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-22)\n",
            "positives (FP) via center points yet cannot solves the scenario\n",
            "that a center point of the third object lies exactly within the\n",
            "center of an FP. However, this scenario often occurs in some\n",
            "datasets, such as aircraft parked side by side in aerial images.\n",
            "CentripetalNet cannot address the object occlusion problem\n",
            "due to the center overlapping, e.g., pedestrian detection sce-\n",
            "nario. In summary, existing heuristic corner grouping meth-\n",
            "ods are dataset (COCO [Lin et al., 2014]) driven with poor\n",
            "generalization, but much less effort has been paid to solve\n",
            "it. To tackle this problem, we propose the Corner AfÔ¨Ånity,\n",
            "whose motivation is to cover all scenarios aforementioned\n",
            "and bring robust grouping capability for corner-guided detec-\n",
            "tor.\n",
            "SpeciÔ¨Åcally, Corner AfÔ¨Ånity embeds three attributes for\n",
            "each target corner, i.e., locations, shapes, and semantic. Lo-\n",
            "cations and shapes are embedded in the structure afÔ¨Ånity (SA)\n",
            "that is a key part of the Corner AfÔ¨Ånity, in which we encode\n",
            "the shape (e.g., width and height) knowledge of each instance\n",
            "in its corner location via a strong supervision manner. SA\n",
            "is a Ô¨Ånal model that is a Ô¨Ånal model that is a Ô¨Ånal model\n",
            "is not enough to accomplish grouping in some extreme sce-\n",
            "narios, such as two objects with similar shapes coincide. To\n",
            "this end, we devise the contexts afÔ¨Ånity (CA) part for Cor-\n",
            "ner AfÔ¨Ånity, in which we utilize the pull-push [Newell et\n",
            "al., 2017; Law and Deng, 2018] algorithm to mine high-level\n",
            "semantic similarity of the corresponding corner pairs to make\n",
            "coincident objects distinguishable. In the CA part, the afÔ¨Ånity\n",
            "of the corner pair is a Ô¨Ånal model that is a Ô¨Ånal model that is\n",
            "self-supervision manner with no need for a real ground-truth\n",
            "value. Besides, to make the two afÔ¨Ånities better interact, we\n",
            "devise the Corner AfÔ¨Ånity function, which runs as coupling\n",
            "CA and SA. In this way, SA and CA interplay so that even if\n",
            "the SA value of two corners belong to different instances is\n",
            "large, CA will decay it to make sure that the value of overall\n",
            "Corner AfÔ¨Ånity is not enough to be used.\n",
            "We select three datasets covering different detection sce-\n",
            "narios, i.e., COCO [Lin et al., 2014], Citypersons [Cords et\n",
            "al., 2016], and UCAS-AOD [Zhu et al., 2015], to verify the\n",
            "effectiveness of our design. Experimental results show that\n",
            "the proposed Corner AfÔ¨Ånity boosts AP of 5.8%, 35.8%, and\n",
            "17.2%, on the above three benchmarks upon CornerNaste-\n",
            "line, proving the solid effectiveness of our design. We hope\n",
            "the efÔ¨Åcient Corner AfÔ¨Ånity can attract more attention to pro-\n",
            "mote the development of bionic corner-guided detector.\n",
            "2\n",
            "Related Work\n",
            "2.1\n",
            "Center-guided Detector\n",
            "Center-guided detectors generally use potential center\n",
            "points/areas, acting as reference positives, to regress the ob-\n",
            "ject bounding box (e.g., height and width). Classical anchor-\n",
            "based detector belongs in this Ô¨Åeld, which regards center as\n",
            "an attribute of the preset anchors. Faster R-CNN [Ren et al.,\n",
            "2015] popularizes the center-guided anchor mechanism in its\n",
            "Region Proposal Network (RPN). The aim of RPN is gener-\n",
            "ating a few of proposals from a set of candidate boxes that\n",
            "are encoded via their centers along with heights and widths,\n",
            "In addition, the design of the centered RPN makes the\n",
            "detector can be end-to-end trainable. Afterwards, the center-\n",
            "driven anchor boxes are widely used in RPN-based two-stage\n",
            "detectors. To further explore the efÔ¨Åciency of models, some\n",
            "center-guided one-stage detectors also appeared. They re-\n",
            "move the RPN and directly do regression and classiÔ¨Åcation\n",
            "at the centers of anchor boxes. For example, SSD [Liu et al.,\n",
            "2016] improves the performance via densely placing center\n",
            "anchors in multiple layers.\n",
            "Unlike the center-gpu and anchor-based detectors [Redmon\n",
            "and Farhadi, 2017; Liao et al., 2016; Lin et al., 2017b], FCOS\n",
            "[Tan et al., 2019] proposes a center-guided anchor-free man-\n",
            "agement strategy to generate the center areas of bounding boxes\n",
            "as positive samples and directly regresses four vectors (the\n",
            "distances from each pixel to the corresponding box‚Äôs bor-\n",
            "ders) without anchor guidance. Instead of four vectors, Rep-\n",
            "points [Yang et al., 2019] predicts a set of representative\n",
            "points in the center area (positive pixels).\n",
            "Despite the center-guided mechanism‚Äôs great success, it is\n",
            "actually difÔ¨Åcult to pinpoint the center of a box. This is be-\n",
            "cause a center requires to be determined via all four bound-\n",
            "aries of the object, needing four degrees of freedom.\n",
            "2.2\n",
            "Corner-guided Detector\n",
            "Corner-guided detectors usually predict corners via out-\n",
            "puting heatmaps, which gave same more bionic in the gen-\n",
            "eration of the object bounding box. Initially, when we want\n",
            "to obtain high-quality bounding boxes manually, each box is\n",
            "used to obtain the center to the bottom-right one and\n",
            "we rarely label the object box by a center point along with\n",
            "height and width. A corner plots used two boundaries (de-\n",
            "grees of freedom) to be determined.\n",
            "CornerNet [Law and Deng, 2018] detects objects by pre-\n",
            "dicting and grouping pairs of corner points. The grouping\n",
            "method in CornerNet is that if a top-left corner and a bottom-\n",
            "right corner belong to the same object, the distance between\n",
            "the center and the vectors will be small. To further optimize\n",
            "this grouping strategy, CenterNet [Duan et al., 2019] adds a\n",
            "prediction branch of corner points based on corners pairs esti-\n",
            "mation, making the center of the object from more complex match-\n",
            "ing. CentripetalNet [Dong et al., 2019] proposes a new cen-\n",
            "triptal grouping approach to matched corner points and\n",
            "achieves state-of-the-art performance.\n",
            "To sum up, corner grouping is meaningful yet challenge\n",
            "for this kind of detector. Our Corner AfÔ¨Ånity aims to produce\n",
            "more robust corner pairs to further advance the development\n",
            "of such human-like corner-guided detector.\n",
            "3\n",
            "Method\n",
            "As shown in Figure 2, the execution of Corner AfÔ¨Ånity re-\n",
            "quires an output of the corresponding corner afÔ¨Ånity map.\n",
            "Each (top-left or bottom-right) map is composed of three\n",
            "channels, i.e., two for encoding the structure afÔ¨Ånity (SA,\n",
            "blue arrow-lines) and one for the contexts afÔ¨Ånity (CA, green\n",
            "arrow-lines). The SA and CA interact via a devised function\n",
            "to be the overall Corner AfÔ¨Ånity. In the following subsections,\n",
            "we will detail the SA, CA, and Corner AfÔ¨Ånity function.\n",
            "3.1\n",
            "The Structure AfÔ¨Ånity\n",
            "The structure afÔ¨Ånity (SA) is a key part of Corner AfÔ¨Ånity,\n",
            "aiming to mine preliminary construction similarity of cor-\n",
            "1459\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0.py --model-name /content/GOT_weights/ --image-file /content/assets_aff2.png --type format --render"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKEVY7UheK_I",
        "outputId": "b2f978b8-9207-4255-fea5-86b66952beb3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-15 20:58:04.570930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-15 20:58:04.603361: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-15 20:58:04.613201: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-15 20:58:05.793615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "<|im_start|>system\n",
            "You should follow the instructions carefully and explain your answers in detail.<|im_end|><|im_start|>user\n",
            "<img><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad></img>\n",
            "OCR with format: <|im_end|><|im_start|>assistant\n",
            "\n",
            "positives (FP) via center points yet cannot solves the scenario that a center point of the third object lies exactly within the center of an FP. However, this scenario often occurs in some datasets, such as aircraft parked side by side in aerial images. CentripetalNet cannot address the object occlusion problem due to the center overlapping, e.g., pedestrian detection scenario. In summary, existing heuristic corner grouping methods or ads dataset (COCO [Lin et al., 2014]) driven with poor generalization, but much less effort has been paid to solve it. To tackle this problem, we propose the Corner Affinity, whose motivation is to cover all scenarios aforementioned and bring robust grouping capability for corner-guided detectors.\n",
            "Specifically, Corner Affinity embeds three attributes for each target corner, i.e., locations, shapes, and semantic. Loactions and shapes are embedded in the structure affinity (SA) that is a key part of the Corner Affinity, in which we encode the shape (e.g., width and height) knowledge of each instance in its corner location via a strong supervision manner. SA is a very simple way to learn the object to the object. We also then set only to accomplish grouping in some extreme scene ratios, such as two objects with similar shapes coincide. The density, and we deÔ¨Åve the contexts affinity (CA) part for Corner Affinity, in which we utilize the pull-push [Newell et al., 2017; Law and Deng, 2018] algorithm to mine high-level semantic similarity of the corresponding corner pairs to make coincident objects distinguishable. In the CA part, the different features of the object are the same as the object, which are self-supervision manner with none need for a real ground-truth value. Besides, to make the two affinities better interact, we devise the Corner Affinity function, which runs as coupling CA and SA. In this way, SA and CA interplay so that even if the SA value of two corners belong to different instances is large, CA will decay it to make sure that the value of overall CGD is not a real ground-truth value.\n",
            "We select three datasets covering different detection scenarios, also, COCO [Lin et al., 2014], Citypersons [Cords et al., 2016], and UCAS-AOD [Zhu et al., 2015], to verify the effectiveness of our design. Experimental results show that the proposed Corner Affinity boosts AP of \\(5.8 \\%, 35.8 \\%\\), and \\(17.2 \\%\\), on the above three benchmarks upon CornerNet baseline, proving the solid effectiveness of our design. We hope the efficient Corner Affinity can attract more attention to promote the development of bionic corner-guided detector.\n",
            "\\section*{2 Related Work}\n",
            "\\subsection*{2.1 Center-guided Detector}\n",
            "Center-guided detectors generally use potential center points/areas, acting as reference positives, to regress the object weight and box (e.g., height and width). Classical anchorbased detector belongs in this field, which regards center as an attribute of the preset anchors. Faster R-CNN [Ren et al., 2015] popularizes the center-guided anchor mechanism in its Region Proposal Network (RPN). The aim of RPN is generating a few of proposals from a set of candidate boxes that are encoded via their centers along with heights and widths. In addition, the design of the centered RPN makes the detector can be end-to-end trainable. Afterwards, the center- driven anchor boxes are widely used in RPN-based two-stage detectors. To further explore the efficiency of models, some center-guided one-stage detectors also appeared. They remove the RPN and directly do regression and classiÔ¨Åcation at the centers of anchor boxes. For example, SSD [Liu et al., 2016] improves the performance via densely placing center anchors in multiple layers.\n",
            "Unlike the center-gpu and anchor-based detectors [Redmon and Farhadi, 2017; Lui et al., 2016; Lin et al., 2017b], FCOS [Tian et al., 2019] proposes a center-guided anchor-free manner to improve pixels in center areas of bounding boxes as positive samples and directly regresses four vectors (the distances from each pixel to the corresponding box's borders) without anchor guidance. Instead of four vectors, Reppoints [Yang et al., 2019] predicts a set of representative points in the center area (positive pixels).\n",
            "Despite the center-guided mechanism's great success, it is actually difficult to pinpoint the center of a box. This is because a center requires to be determined via all four boundaries of the object, needing four degrees of freedom.\n",
            "\\subsection*{2.2 Corner-guided Detector}\n",
            "Corner-guided detectors usually predict corners via outputting heatmaps, which gave same more bionic in the generation of the object bounding box. Initially, when we want to obtain high-quality bounding boxes manually, each box is then connected to the bottom-right one and we rarely label the object box by a center point along with height and width. A corner-based two boundaries (decrees of freedom) to be determined.\n",
            "CornerNet [Law and Deng, 2018] detects objects by predicting and grouping pairs of corner points. The grouping method in CornerNet is that if a top-left corner and a bottomright corner belong to the same object, the distance between the corner points vectors will be small. To further optimize this grouping structure, CenterNet Duan et al., 2019] adds a prediction branch of corner points based on corners pairs estimation, making the correct image become triplets matching. CentripetalNet [Dong et al., 2019] proposes a new centerful grouping approach to matched areured corner points and achieves state-of-the-art performance.\n",
            "To sum up, corner grouping is meaningful yet challenge for this kind of detector. Our Corner Affinity aims to produce more robust corner pairs to further advance the development of such human-like corner-guided detector.\n",
            "\\section*{3 Method}\n",
            "As shown in Figure 2, the execution of Corner Affinity requires an output of the corresponding corner affinity maps. Each (top-left or bottom-right) map is composed of three channels, i.e., two for encoding the structure affinity (SA, blue arrow-lines) and one for the contexts affinity (CA, green arrows lines). The SA and CA interact via a devised function to be the overall Corner Affinity. In the following subsections, we will detail the SA, CA, and Corner Affinity function.\n",
            "\\subsection*{3.1 The Structure Affinity}\n",
            "The structure affinity (SA) is a key part of Corner Affinity, aiming to mine preliminary construction similarity of cor-\n",
            "==============rendering===============\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0.py\", line 252, in <module>\n",
            "    eval_model(args)\n",
            "  File \"/content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0.py\", line 202, in eval_model\n",
            "    with open(html_path, 'r') as web_f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './render_tools//content-mmd-to-html.html'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0.py --model-name /content/GOT_weights/ --image-file /content/assets_aff2.png --type format --render"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIO8kY1venZo",
        "outputId": "3aaee943-45ca-4cc1-e05a-f5332c207b32"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-15 21:05:36.492437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-15 21:05:36.512092: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-15 21:05:36.518644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-15 21:05:37.681030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "<|im_start|>system\n",
            "You should follow the instructions carefully and explain your answers in detail.<|im_end|><|im_start|>user\n",
            "<img><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad><imgpad></img>\n",
            "OCR with format: <|im_end|><|im_start|>assistant\n",
            "\n",
            "positives (FP) via center points yet cannot solves the scenario that a center point of the third object lies exactly within the center of an FP. However, this scenario often occurs in some datasets, such as aircraft parked side by side in aerial images. CentripetalNet cannot address the object occlusion problem due to the center overlapping, e.g., pedestrian detection scenario. In summary, existing heuristic corner grouping methods or ads dataset (COCO [Lin et al., 2014]) driven with poor generalization, but much less effort has been paid to solve it. To tackle this problem, we propose the Corner Affinity, whose motivation is to cover all scenarios aforementioned and bring robust grouping capability for corner-guided detectors.\n",
            "Specifically, Corner Affinity embeds three attributes for each target corner, i.e., locations, shapes, and semantic. Loactions and shapes are embedded in the structure affinity (SA) that is a key part of the Corner Affinity, in which we encode the shape (e.g., width and height) knowledge of each instance in its corner location via a strong supervision manner. SA is a very simple way to learn the object to the object. We also then set only to accomplish grouping in some extreme scene ratios, such as two objects with similar shapes coincide. The density, and we deÔ¨Åve the contexts affinity (CA) part for Corner Affinity, in which we utilize the pull-push [Newell et al., 2017; Law and Deng, 2018] algorithm to mine high-level semantic similarity of the corresponding corner pairs to make coincident objects distinguishable. In the CA part, the different features of the object are the same as the object, which are self-supervision manner with none need for a real ground-truth value. Besides, to make the two affinities better interact, we devise the Corner Affinity function, which runs as coupling CA and SA. In this way, SA and CA interplay so that even if the SA value of two corners belong to different instances is large, CA will decay it to make sure that the value of overall CGD is not a real ground-truth value.\n",
            "We select three datasets covering different detection scenarios, also, COCO [Lin et al., 2014], Citypersons [Cords et al., 2016], and UCAS-AOD [Zhu et al., 2015], to verify the effectiveness of our design. Experimental results show that the proposed Corner Affinity boosts AP of \\(5.8 \\%, 35.8 \\%\\), and \\(17.2 \\%\\), on the above three benchmarks upon CornerNet baseline, proving the solid effectiveness of our design. We hope the efficient Corner Affinity can attract more attention to promote the development of bionic corner-guided detector.\n",
            "\\section*{2 Related Work}\n",
            "\\subsection*{2.1 Center-guided Detector}\n",
            "Center-guided detectors generally use potential center points/areas, acting as reference positives, to regress the object weight and box (e.g., height and width). Classical anchorbased detector belongs in this field, which regards center as an attribute of the preset anchors. Faster R-CNN [Ren et al., 2015] popularizes the center-guided anchor mechanism in its Region Proposal Network (RPN). The aim of RPN is generating a few of proposals from a set of candidate boxes that are encoded via their centers along with heights and widths. In addition, the design of the centered RPN makes the detector can be end-to-end trainable. Afterwards, the center- driven anchor boxes are widely used in RPN-based two-stage detectors. To further explore the efficiency of models, some center-guided one-stage detectors also appeared. They remove the RPN and directly do regression and classiÔ¨Åcation at the centers of anchor boxes. For example, SSD [Liu et al., 2016] improves the performance via densely placing center anchors in multiple layers.\n",
            "Unlike the center-gpu and anchor-based detectors [Redmon and Farhadi, 2017; Lui et al., 2016; Lin et al., 2017b], FCOS [Tian et al., 2019] proposes a center-guided anchor-free manner to improve pixels in center areas of bounding boxes as positive samples and directly regresses four vectors (the distances from each pixel to the corresponding box's borders) without anchor guidance. Instead of four vectors, Reppoints [Yang et al., 2019] predicts a set of representative points in the center area (positive pixels).\n",
            "Despite the center-guided mechanism's great success, it is actually difficult to pinpoint the center of a box. This is because a center requires to be determined via all four boundaries of the object, needing four degrees of freedom.\n",
            "\\subsection*{2.2 Corner-guided Detector}\n",
            "Corner-guided detectors usually predict corners via outputting heatmaps, which gave same more bionic in the generation of the object bounding box. Initially, when we want to obtain high-quality bounding boxes manually, each box is then connected to the bottom-right one and we rarely label the object box by a center point along with height and width. A corner-based two boundaries (decrees of freedom) to be determined.\n",
            "CornerNet [Law and Deng, 2018] detects objects by predicting and grouping pairs of corner points. The grouping method in CornerNet is that if a top-left corner and a bottomright corner belong to the same object, the distance between the corner points vectors will be small. To further optimize this grouping structure, CenterNet Duan et al., 2019] adds a prediction branch of corner points based on corners pairs estimation, making the correct image become triplets matching. CentripetalNet [Dong et al., 2019] proposes a new centerful grouping approach to matched areured corner points and achieves state-of-the-art performance.\n",
            "To sum up, corner grouping is meaningful yet challenge for this kind of detector. Our Corner Affinity aims to produce more robust corner pairs to further advance the development of such human-like corner-guided detector.\n",
            "\\section*{3 Method}\n",
            "As shown in Figure 2, the execution of Corner Affinity requires an output of the corresponding corner affinity maps. Each (top-left or bottom-right) map is composed of three channels, i.e., two for encoding the structure affinity (SA, blue arrow-lines) and one for the contexts affinity (CA, green arrows lines). The SA and CA interact via a devised function to be the overall Corner Affinity. In the following subsections, we will detail the SA, CA, and Corner Affinity function.\n",
            "\\subsection*{3.1 The Structure Affinity}\n",
            "The structure affinity (SA) is a key part of Corner Affinity, aiming to mine preliminary construction similarity of cor-\n",
            "==============rendering===============\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0.py\", line 252, in <module>\n",
            "    eval_model(args)\n",
            "  File \"/content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0.py\", line 202, in eval_model\n",
            "    with open(html_path, 'r') as web_f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './render_tools/content-mmd-to-html.html'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0_crop.py --model-name /content/GOT_weights/ --image-file /content/multicrop  --multi-page"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sl_KQwPOeut3",
        "outputId": "4ebdf8e8-7023-4750-ac63-878caab99955"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-15 21:37:14.073281: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-15 21:37:14.093130: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-15 21:37:14.099349: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-15 21:37:15.304915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0_crop.py\", line 251, in <module>\n",
            "    eval_model(args)\n",
            "  File \"/content/GOT-OCR2.0/GOT-OCR-2.0-master/GOT/demo/run_ocr_2.0_crop.py\", line 159, in eval_model\n",
            "    image_list = torch.stack(image_list)\n",
            "RuntimeError: stack expects a non-empty TensorList\n"
          ]
        }
      ]
    }
  ]
}